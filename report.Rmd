---
title: "Toronto Major Crime Indicators - Clustering"
author: "TALL Machine Learning - Iman Lau, Dung Tran, Zheng (James) Lai"
date: "October 12, 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r warning=FALSE,message=FALSE}
library(ggplot2)
library(dplyr)
library(ggmap)
library(tidyr)
library(cluster)
library(lubridate)
library(rgl)
library(maptools)
library(ggpubr)
library(viridis)
library(tidyverse)
library(scatterplot3d)
library(factoextra)
library(fpc)
library(NbClust)
library(reshape2)
library(scales)
library(plyr)
library(caret)
library(ROCR)
library(pROC)
library(rlist)
library(Hmisc)
library(corrplot)
library(ggcorrplot)
library(DMwR)
library(ggthemes)
library(e1071)
library(maps)
library(RColorBrewer)
library(rgl)

model_dir = "models"
data_dir = "data"

### Loading data
data=read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
### Loading map
tormap <- get_map(location =c(left=-79.8129, bottom=43.4544, right=-78.9011, top=43.9132))
torontoMap <- ggmap(tormap)
```

## I. Introduction

The second assignment for the York Machine Learning course, *Machine Learning in Business Context* was to explore unsupervised machine learning algorithms, specifically clustering. We chose a dataset from the Toronto Police Sercive Public Safety Data Portal, [MCI 2014 to 2017](http://data.torontopolice.on.ca/datasets/mci-2014-to-2017). This report follows the structures laid out in CRISP-DM methodology.

The GitHub repository for all the source code is located at the following link: [link here](link here).

The RShiny app is located at the following link: [link here](link here).

## II. Business Understanding

The [Toronto Police Service](https://en.wikipedia.org/wiki/Toronto_Police_Service) is the police force that serves the Greater Toronto Area. It is the largest municipal police force in Canada and the third largest police force in Canada. They are a taxpayer-funded service, ranking as second for the government of Toronto's budgetary expenses. 

The objective of this model is to cluster crimes to determine which areas of Toronto have the most levels of crime, overall and for different Major Crime Indicators. This would enable the Toronto Police to most effectively allocate their officers and specialists to the areas that require them the most. The hope is that this would be an effective way to lower crime rates and enable more cases to be solved, all without spending more money.

There are some ethical implications of using crime data. There are many avenues for bias to enter the data set. Police services around North America have come under increased scrutiny in recent years for racist policing. Some police policies or laws inherently disadvantage certain groups of people, which would create bias in the data. This means that conclusions drawn from a machine learning model based on biased data would create biased results. The conclusions should be looked at with other data, such as demographic data, and supplementary information, such as social considerations.

## III. Data Understanding

The data set was provided courtesy of the [Toronto Police Service Open Data Portal](http://data.torontopolice.on.ca/). It is usable in accordance with the [Open Government License - Ontario](https://www.ontario.ca/page/open-government-licence-ontario).

The data concerns all Major Crime Indicators (MCI) occurrences in Toronto by reported date, between 2014 and 2017. The MCIs are Assault, Break and Enter, Auto Theft, and Theft Over (excludes Sexual Assaults). Locations in the data set are approximate, as they have been deliberately offset to the nearest road intersection to protect the privacy of involved individuals. 

There are 29 columns with 131,073 observations. However, 4 of these columns are duplicates, bringing us to 25 columns. There are many attributes: date/time-type data, crime-type data, and location type data.

The following table shows the date/time attributes:


Attribute              | Description
-----------------------|------------------------------------------------------------------------
Occurrence Date        | Date of occurrence
Occurrence Year        | Year of occurrence
Occurrence Month       | Month of occurrence
Occurrence Day         | Day of occurrence
Occurrence Day of Year | Day of year of occurrence 
Occurrence Day of Week | Day of week of occurrence
Occurrence Hour        | Hour of occurrence
Reported Date          | Date of report
Reported Year          | Year of report
Reported Month         | Month of report
Reported Day           | Day of report
Reported Day of Year   | Day of year of report 
Reported Day of Week   | Day of week of report
Reported Hour          | Hour of report


While this table shows the categorical attributes:

Attribute             | Description
----------------------|------------------------------------------------------------------------
Premise Type          | The type of premise (outside, house, commercial, etc.) where the crime happened
Major Crime Indicator | The major crime indicator (type of crime)
Offence               | The specific offence that happened
UCR Code              | Uniform Crime Reporting Code - categorizes the crime
UCR Extension         | Uniform CRime Reporting Extension - further categorizes the crime

This table shows location attributes:

Attribute     | Description
--------------|------------------------------------------------------------------------
Division      | The Toronto Police Division
Neighbourhood | Toronto Neighbourhood
Hood ID       | Toronto neighbourhood ID
Lat           | Latitude
Long          | Longitude

## IV. Data Exploration and Preparation

### (1) Visualizations for MCI (Major Crime Indicators) and date-related attributes

We can observe the Toronto criminals based on MCI and occurrence date-related attributes, which in this dataset are occurrences' time, day of week, month and year cross Toronto or on a specific division or neighbourdhood in Toronton. The following visualizations are for total Toronto, please check our application for more option.

```{r}
toronto <- subset(data, !duplicated(data$event_unique_id))
toronto <- subset(toronto, !is.na(toronto$occurrenceyear))
toronto <- subset(toronto, !is.na(toronto$offence))
toronto <- subset(toronto, !(is.na(toronto$occurrencedayofweek) | toronto$occurrencedayofweek == ""))
neighbourhoods  <-  unique(toronto$Neighbourhood)

# Crimes by MCI in Toronto
crime_group <- group_by(toronto, MCI)
crime_by_type <- dplyr::summarise(crime_group, Occurrences=n())
crime_by_type <- crime_by_type[order(crime_by_type$Occurrences, decreasing = TRUE),]
ggplot(aes(x = reorder(MCI, Occurrences) , y = Occurrences), data = crime_by_type) +
      geom_bar(stat = 'identity', position = position_dodge(), width = 0.5) +
      geom_text(aes(label = Occurrences), stat = 'identity', data = crime_by_type, hjust = 1.1, size = 3.5, color = "white") +
      coord_flip() +
      xlab('Major Crime Indicators') +
      ylab('Number of Occurrences') +
      theme_minimal() +
      theme(plot.title = element_text(size = 16),
            axis.title = element_text(size = 12, face = "bold"),
            axis.text.x = element_text(angle = 90, hjust = 1, vjust = .4))
# Crimes by hour
option_group <- group_by(toronto, occurrencehour, MCI)
option_crime <- dplyr::summarise(option_group, n=n())
ggplot(aes(x=occurrencehour, y=n, color=MCI), data =option_crime) + 
        geom_line(size=1.5) + 
        ylab('Number of Occurrences') +
        xlab('Hour(24-hour clock)') +
        theme_grey() +
        theme(plot.title = element_text(size = 16),
              axis.title = element_text(size = 12, face = "bold"))
# Crimes by day of week
day_group <- group_by(toronto, occurrencedayofweek, MCI)
day_count <- dplyr::summarise(day_group,Total = n())
ggplot(day_count, aes(occurrencedayofweek, MCI, fill = Total)) +
        geom_tile(size = 1, color = "white") +
        scale_fill_viridis()  +
        geom_text(aes(label=Total), color='white') +
        xlab('Day of Week') +
        theme(plot.title = element_text(size = 16), 
              axis.title = element_text(size = 12, face = "bold"))
# Crimes by month
month_group <- group_by(toronto, occurrencemonth, MCI) 
month_count <- dplyr::summarise(month_group, n=n())
month_count$occurrencemonth <- ordered(month_count$occurrencemonth, levels = c('January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'))
ggplot(aes(x = occurrencemonth, y = n, fill = MCI), data = month_count) +
        geom_bar(stat = 'identity',position = "stack", width = 0.6) +
        coord_flip() +
        xlab('Month') +
        ylab('Number of Occurrences') +
        theme_bw() +
        theme(plot.title = element_text(size = 16),
              axis.title = element_text(size = 12, face = "bold"))
# Crimes by year
year_data <- subset(toronto,!(toronto$occurrenceyear < 2014))
option_group <- group_by(year_data, occurrenceyear, MCI)
option_crime <- dplyr::summarise(option_group, n=n())
ggplot(aes(x=occurrenceyear, y=n, color=MCI), data =option_crime) + 
        geom_line(size=1.5) + 
        ylab('Number of Occurrences') +
        xlab('Year)') +
        theme_grey() +
        theme(plot.title = element_text(size = 16),
              axis.title = element_text(size = 12, face = "bold"))

```

### (2) Visualizations for the division and neighbourhood attributes

Visualization crime level for all neighbourhoods.

```{r}
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% dplyr::summarise(offence_cnt = n())
hood_total_offence_cnt_table = merge(total_offence_cnt_table,sh@data,by.x='Hood_ID',by.y='AREA_S_CD')
points_offense_cnt <- fortify(sh, region = 'AREA_S_CD')
points_offense_cnt <- merge(points_offense_cnt, hood_total_offence_cnt_table, by.x='id', by.y='Hood_ID', all.x=TRUE)
torontoMap + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points_offense_cnt, color='black') +
  scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
```

Visualization Toronto Crimes Heatmap across neighboughhoods

```{r}
base_size <- 9
    heat_group <- group_by(toronto, Neighbourhood, offence)
    heat_count <- dplyr::summarise(heat_group,Total = n())
    heat_count$Neighbourhood <- with(heat_count,reorder(Neighbourhood,Total))
    heat_count.m <- melt(heat_count)
    heat_count.m <- ddply(heat_count.m, .(variable), transform,rescale = rescale(value))
    ggplot(heat_count.m, aes(Neighbourhood, offence)) + 
        geom_tile(aes(fill = rescale),colour = "white") + 
        ggtitle("Toronto Criminal Heatmap") +
        scale_fill_gradient(low = "lightblue",high = "darkblue") +
        theme_grey(base_size = base_size) + 
        labs(x = "", y = "") + 
        scale_x_discrete(expand = c(0, 0)) +
        scale_y_discrete(expand = c(0, 0)) +
        theme_minimal() +
        theme(legend.position = "none",axis.ticks = element_blank(), axis.text.x = element_text(size = base_size *0.5, angle = 270, hjust = 0, colour = "grey50"),axis.text.y = element_text(size = base_size *0.5))
```

We also can visualize some interesting information from the dataset e.g.: Finding the top dangerous or top safe zones, neighbourhoods in Toronto based on all MCI or on a specific MCI. The following charts display top 5 dangerous and top 5 safe neighbourhoods in Toronto on total MCI. For more options on other top neighbourhoods/divisions on total MCI or for a specific MCI, please explore on our application.

```{r}
location_group <- group_by(toronto, Neighbourhood)
crime_by_location <- dplyr::summarise(location_group, n=n())
crime_dangerous <- crime_by_location[order(crime_by_location$n, decreasing = TRUE), ]
crime_dangerous_top <- head(crime_dangerous, 5)
ggplot(aes(x = reorder(Neighbourhood, n), y = n, fill = reorder(Neighbourhood, n)), data = crime_dangerous_top) +
          geom_bar(stat = 'identity', width = 0.6) +
          ggtitle("Top 5 dangerous neighbourhoods in Toronto") +
          coord_flip() +
          xlab('Zone') +
          ylab('Number of Occurrences') +
          scale_fill_brewer(palette = "Reds") +
          theme(plot.title = element_text(size = 16),
                axis.title = element_text(size = 12, face = "bold"),
                legend.position="none")

crime_safe <-  crime_by_location[order(crime_by_location$n, decreasing = FALSE), ]
crime_safe_top <- head(crime_safe, 5)
ggplot(aes(x = reorder(Neighbourhood, -n), y = n, fill = reorder(Neighbourhood, -n)), data = crime_safe_top) +
        geom_bar(stat = 'identity', width = 0.6) +
        ggtitle("Top 5 safe neighbourhoods in Toronto") +
        coord_flip() +
        xlab('Zone') +
        ylab('Number of Occurrences') +
        scale_fill_brewer(palette = "Greens") +
        theme(plot.title = element_text(size = 16),
              axis.title = element_text(size = 12, face = "bold"),
              legend.position="none")
```

### (3) Data outliers

It seems like someone reported crimes over 40 years ago. But since nothing stops people from doing this, we are not going to treat these as outliers.

```{r}
data$occurrencedate <- ymd(gsub("(.*)T.*", "\\1", data$occurrencedate))
data$reporteddate <- ymd(gsub("(.*)T.*", "\\1", data$reporteddate))
data[which(data$occurrencedate < as.POSIXct("1970-01-01")),]
```

The rest of the data is mostly categorical in their nature, so no concerns need to be addressed in terms of data outliers here.

### (4) Data Preprocessing

The following code documents the preprocessing steps idendified for this dataset.

```{r warning=FALSE,message=FALSE}
#reload the data
data <- read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
```

First, we want to remove any duplicate data - rows or columns. Some events have duplicated event IDs and should be removed. We also have duplicate columns for X/Y and Lat/Long, which should be removed. We are don't use the UCR codes or the ID numbers, so they're also removed.

```{r warning=FALSE,message=FALSE}
#remove duplicate rows/entries
data <- subset(data, !duplicated(data$event_unique_id))

#remove columns that aren't used/duplicates
data <- data[, !colnames(data) %in% c("?..X","Y","Index_","event_unique_id","ucr_code","ucr_ext","FID")]

```

Next, we format the dates. There are garbage time values at the end of the dates, which are removed. The other date values are also changed into appropriate date/time values. Whitespace is also present in the day of week columns, so that is trimmed.

```{r warning=FALSE,message=FALSE}
#formatting dates - remove garbage time values at the end
data$occurrencedate <- gsub("(.*)T.*", "\\1", data$occurrencedate)
data$reporteddate <- gsub("(.*)T.*", "\\1", data$reporteddate)
data$occurrencetime = ymd_h(paste(data$occurrencedate,data$occurrencehour,sep = " "), tz="EST")
data$reportedtime = ymd_h(paste(data$reporteddate,data$reportedhour,sep = " "), tz="EST")
data$occurrencedate = ymd(data$occurrencedate)
data$reporteddate = ymd(data$reporteddate)

#removing whitespace from day of week
data$occurrencedayofweek <- as.factor(trimws(data$occurrencedayofweek, "b"))
data$reporteddayofweek <- as.factor(trimws(data$reporteddayofweek, "b"))
```

We also convert the month/day of week from string representation to integer representation:

```{r}
data$reportedmonth = month(data$reportedtime)
data$reporteddayofweek = wday(data$reportedtime)
data$occurrencemonth = month(data$occurrencetime)
data$occurrencedayofweek = wday(data$occurrencetime)
```

Now let's take a look at the missing data:

```{r warning=FALSE,message=FALSE}
#missing data
colSums(is.na(data))
NAdata <- unique (unlist (lapply (data, function (x) which (is.na (x)))))
```

Rows with NA data:

```{r warning=FALSE,message=FALSE}
NAdata
```

Occurence dates for rows with NA data:

```{r warning=FALSE,message=FALSE}
data$occurrencedate[NAdata]
```

We can see that there are 32 missing values, all in occurrence date related columns. Upon further inspection, these are all the same rows. We can also see that the occurrence date value is correct, so these date type columns can have their missing values imputed. 

```{r warning=FALSE,message=FALSE}
#imputing occurence dates from occurence date field
data$occurrenceyear[NAdata] <- year(data$occurrencedate[NAdata])
data$occurrencemonth[NAdata] <- month(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrenceday[NAdata] <- day(data$occurrencedate[NAdata])
data$occurrencedayofweek[NAdata] <- wday(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrencedayofyear[NAdata] <- yday(data$occurrencedate[NAdata])
```

Now we replace the space in the strings with an underscore for easier processing:

```{r warning=FALSE,message=FALSE}
#replace space in string
data$offence <- gsub("\\s", "_", data$offence)
data$MCI <- gsub("\\s", "_", data$MCI)
```

Next, all columns are converted into factors except Lat, Long, reportedtime, and occurrencetime. Unused factor levels are also dropped (resulted from missing values).

```{r warning=FALSE,message=FALSE}

#change things to factors
for(col in c("offence","MCI","Division","Hood_ID")) {
  data[,col] = as.factor(data[,col])
}

#if we use the gower distance and daisy() function, the following metrics can be considered to converted to ordered factor; but since gower distance turns out to be too expensive for large dataset, we have treated the following as normal factors as well!
for(col in c("reportedyear","reportedmonth","reportedday","reporteddayofyear","reporteddayofweek",
             "reportedhour","occurrenceyear","occurrencemonth","occurrenceday","occurrencedayofyear",
             "occurrencedayofweek","occurrencehour")) {
  data[,col] = factor(data[,col])
}

#drop unused factor levels
for(col in names(data)) {
  if(is.factor(data[,col])) {
    data[,col] = droplevels(data[,col])
  }
}
```

Finally, we cherck for missing data one last time:

```{r warning=FALSE,message=FALSE}
# Check missing values one last time
colSums(is.na(data))
```

## V. Modeling

With the data prepared, we can now start looking at models. 

### (1) Clustering strategy I.
First, we can look at clustering crime by neighborhood. We need to coerce the data into a clusterable table, sorted by MCI and neighborhood.

```{r}
# Neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Hood_ID)
groups <- dplyr::summarise(bygroup, n=n())
groups <- groups[c("Hood_ID", "MCI", "n")]
hood <- as.data.frame(spread(groups, key=MCI, value=n))
hood_id = as.integer(hood[,"Hood_ID"])
hood = hood[,-1]
head(hood)
```

Then we can use this table to perform k-means clustering. First, we need to normalize the data and determine the number of clusters. To determine the number of clusters, we simply plot the within-cluster sum-of-squares and pick a number after inspecting this plot.

```{r}
hood <- scale(hood)
#determine number of clusters
wssplot <- function(data, nc=15, seed=1234) {
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc) {
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)
    }
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}

#we can see there's an elbow around 3 clusters
wssplot(hood, nc=15)
```

Now it seems number 3 is a good choice of the number of clusters. We then proceed to perform K-means clustering on the data-set. We have followed this tutorial(https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/) to evaluate the quality of clustering results. Specifically, we have used Silhouette width as a quantitative measures to evaluate the clustering quality. As we will see in the Silhouette plot, both in cluster #2 and cluster #3 have data points where the Silhouette width falls to negative, indicating a not so ideal clustering. This can also be observed in the 1st plot where we see that cluster #2 and cluster #3 is quite close to each other.

```{r}
# K-means clustering
km.res <- eclust(hood, "kmeans", k = 3, graph = T)
fviz_silhouette(km.res, palette = "jco", ggtheme = theme_classic())
```

Let's look at the cluster means from the outputs to see what we can learn.

```{r}
# k-means results
km.res
```

We can see that cluster 3 has lower than average crime. It has 93 neighborhoods, meaning that the majority of Toronto is quite safe in terms of the number of incidents occurring. Cluster 2 has slightly above average crime numbers, and it only has 37 neighborhoods. 10 neighborhoods have much higher than average crime numbers, which is seen in cluster 3. In some way, we can interpret the results as such that crime is concentrated in these small pockets of Toronto.

As an interesting activity, we also looking at a 3D representation of the clustering results. With 3 principal components, more variations should be captured that as in the case in 2 principal components. With a third dimension, we can see that there is more of a spread than initially can be seen simply in two dimensions.

```{r}
#3D plot
pc <-princomp(hood, cor=TRUE, scores=TRUE)
plot3d(pc$scores[,1:3], col=km.res$cluster, main="k-means clusters")
```

Next, we can look at the neighborhoods from a hierarchical clustering approach. Again, we need to determine how many clusters we want to have. For hierarchical clustering, we look at the total number of observations that end up in different clusters with different configurations(in this case, the configuration is the number of clusters).

```{r}
counts <- sapply(2:6, function(ncl) eclust(hood, "hclust", k = ncl, hc_metric = "euclidean", hc_method = "ward.D2")$size)
names(counts) <- 2:6
counts
```

Still, we can see that 3 clusters are not bad if we consider only the cluster size as the criteria for choosing the number of clusters. We don't want to split the neighborhoods into clusters with a small number of neighborhoods.

Now that we have chosen the number of clusters, we proceed to the clustering process. As previously in k-means, we look at Silhouette width as a quantitative measure to evaluate the clustering quality. To visually inspect the clusters, we plot dendrograms.

```{r}
# Hierarchical clustering
hc.res <- eclust(hood, "hclust", k = 3, hc_metric = "euclidean", 
                 hc_method = "ward.D2")

# Visualize dendrograms
fviz_dend(hc.res, show_labels = FALSE,
         palette = "jco", as.ggplot = TRUE)

fviz_silhouette(hc.res, palette = "jco", ggtheme = theme_classic())
```

It seems like we have reached the same conclusion as the k-means algorithms: cluster #1 and cluster #2 does not have large gap. The following codes plot the clustering results on the map. We also plot the offence count in each neighbourhood and it can be seen that the 2nd plot is highly correlated with the 1st plot (which makes sense).

```{r}
cluster_ids <- km.res$cluster
hood_ids_and_cluster_ids <- data.frame(cbind(hood_id,cluster_ids))
hood_ids_and_cluster_ids$cluster_ids = as.factor(hood_ids_and_cluster_ids$cluster_ids)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)

hood_name_and_cluster_ids = merge(hood_ids_and_cluster_ids,sh@data,by.x='hood_id',by.y='AREA_S_CD')
points_clustering <- fortify(sh, region = 'AREA_S_CD')
points_clustering <- merge(points_clustering, hood_name_and_cluster_ids, by.x='id', by.y='hood_id', all.x=TRUE)

p1 = torontoMap + geom_polygon(aes(x=long,y=lat, group=group, fill=cluster_ids), data=points_clustering, color='black') +
  scale_fill_brewer(palette = "Set2")
p2 = torontoMap + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points_offense_cnt, color='black') +
  scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
p3 = ggarrange(p1, p2, ncol = 2, nrow = 1, common.legend = F)
print(p3)
```

### (2) Clustering strategy II.

We can perform clustering on lat/long to look at natural crime hotspots. We can compare this to a manual cluster like the Toronto police divisions.

```{r}
latlong <- data[, colnames(data) %in% c("Lat", "Long")]

# k-means 
# 34 divisions in Toronto
k.means.fit <- kmeans(latlong, 34)
torclus <- as.data.frame(k.means.fit$centers)
torclus$size <- k.means.fit$size
latlongclus <- latlong
latlongclus$cluster <- as.factor(k.means.fit$cluster)
tormap <- get_map(location =c(left=-79.8129, bottom=43.4544, right=-78.9011, top=43.9132))

#plot each incident, color-coded by cluster
ggmap(tormap) +
  geom_point( data= latlongclus, aes(x=Long[], y=Lat[], color= as.factor(cluster))) +
  theme_void() + coord_map() 

#plot bubble map with cluster centroids, size/ color determined by the number of incidents in each cluster
ggmap(tormap) +
  geom_point( data= torclus, aes(x=Long[], y=Lat[], size=size)) +
  theme_void() + coord_map() 
```

We can see how the clusters look with both the incidents plotted as well as the centroids. Next, we can perform k-means clustering on the natural crime hotspot clusters and on the Toronto Police divisions to see how they compare to one another.

```{r}
#coerce data for Hotspots
data2 <- data
data2$cluster <- k.means.fit$cluster
bygroup <- group_by(data2, MCI, cluster)
groups <- dplyr::summarise(bygroup, n=n())
groups <- groups[c("cluster", "MCI", "n")]
hotspot <- data.frame(spread(groups, key=MCI, value=n))
hotspot <- hotspot[, -1]
hotspot = data.frame(scale(hotspot))
#determine number of clusters
#we can see there's an elbow around 4 clusters
wssplot(hotspot, nc=15)
```

We can see here that there is an elbow around 4, so we can run k means with 4 clusters. With these 4 clusters, we can see that cluster 4 has higher than average auto thefts, and slightly higher robberies, but is below average for the other 3 MCIs. Cluster 1 is a safe part of Toronto with lower crime incidents. Cluster 3 has slightly more assaults, break and enters, and robberies, but lower auto thefts and theft overs. Cluster 2, however, is the most crime-centric area of Toronto, with many more incidents than average, excepting auto thefts. Luckily, there are only 3 hotspots in cluster 2, while there are 12 and 13 in clusters 1 and 3 respectively, which had generally fewer incidents overall. 

From the silhouette plot, we can see that clustering based on this data set is a little better as compared to Clustering strategy I. When we plot the clusters using principal component analysis in 2D, we can see that the clusters are relatively well separated. Cluster 1 and 4 are spread out, but clusters 2 and 3 are tighter. On the map, we can also see that cluster 4 is concentrated in downtown Toronto, while cluster 1 is in the northwest part. 

```{r}
km.res <- eclust(hotspot, "kmeans", k = 4, graph = T)
fviz_silhouette(km.res, palette = "jco", ggtheme = theme_classic())
km.res
hotspot$cluster <- factor(km.res$cluster)
ggmap(tormap) +
  geom_point( data= torclus, aes(x=Long[], y=Lat[], color = hotspot$cluster)) +
  theme_void() + coord_map()
```

### (3) Clustering strategy III.

The following code documents the third way we have tried in terms of clustering. Instead of aggregating the dataset based on neighbourhood, we can aggregate the dataset according to the police division that each event belongs to.

```{r}
#coerce data for Divisions
bygroup <- group_by(data, MCI, Division)
groups <- dplyr::summarise(bygroup, n=n())
groups <- groups[c("Division", "MCI", "n")]
div <- as.data.frame(spread(groups, key=MCI, value=n))
div_name = div[,1]
div <- div[, -1]

#normalize
for(col in names(div)) {
  div[,col] = (div[,col] - mean(div[,col])) / sd(div[,col])
}

#determine number of clusters
#we can see there's an elbow around 3 clusters
wssplot(div, nc=15)

# k-means
km.res <- eclust(div, "kmeans", k = 3, graph = T)
fviz_silhouette(km.res, palette = "jco", ggtheme = theme_classic())
km.res
```

Similar to clustering after neighborhood-based aggregation, two clusters have low crime incidents (1 and 2), while cluster 3 has higher crime incidents. Most districts are lower crime incident districts, while 9 districts are higher. However, from both the visual representation (based on principal components) of the clustering and silhouette plot, police-division-based aggregation clearly works better than neighborhood-based aggregation.

### (3) Clustering strategy IV.

So far we have aggregated crime statistics into either neibourhood or division. What if we run clustering on data that excludes geographical information. Does such clustering produce naturally occuring crime zone? We will find out in the following analysis.

```{r}
library(clustMixType)
data_fullclust = data[,c("occurrencetime","reportedtime","premisetype","offence","MCI")]
data_fullclust$occurrencetime = as.numeric(data_fullclust$occurrencetime)
data_fullclust$reportedtime = as.numeric(data_fullclust$reportedtime)
```

However, one important distinction as compared to previous clustering analysises is that, we now have mixed data type, being numeric and categorical. To handle this situation, we have experimented using the daisy function and Gower's distance formula. However, Gower's distance formula drastically increase memory consumption when the dataset is large, making it not pratical to run on our own computers. As such, we swtiched to another algorithms "k-prototypes clustering" that can handle mixed type data. As in kmeans, we determine the number of clusters based on the trends shown in within cluster sum of squares.

```{r}
if (file.exists(paste(model_dir,"kproto_clusters.dat",sep="/"))) {
  kproto_clusters = load(paste(model_dir,"kproto_clusters.dat",sep="/"))
} else {
  library(rlist)
  set.seed(1234)
  kproto_clusters = list()
  for (i in seq(2,8,1)) {
    kproto_clusters = list.append(kproto_clusters,kproto(data_fullclust,i,lambda = 1))
  }
  save(kproto_clusters,file=paste(model_dir,"kproto_clusters.dat",sep="/"))
}
```

```{r}
wss = c()
for(kc in kproto_clusters) {
  wss = c(wss,kc$tot.withinss)
}
plot(seq(2,8,1),wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
```

We see here that, beyond 4 clusters (or 7 clusters), there seems to be diminishing returns in reducing within-cluster sum of squres. We choose 4 clusters in this case. Then plot the clustering results on a map:

```{r}
kproto_selection = kproto_clusters[[3]]
data$cluster_id = as.factor(kproto_selection$cluster)
to_map <- data.frame(data$MCI, data$Lat, data$Long, data$cluster_id)
colnames(to_map) <- c('crimes', 'lat', 'lon', 'cluster_id')
sbbox <- make_bbox(lon = data$Long, lat = data$Lat, f = 0.05)
my_map <- get_map(location = sbbox, maptype = "roadmap", scale = 2, color="color", zoom = 10)
ggmap(my_map) +
  geom_point(data=to_map, aes(x = lon, y = lat, color = cluster_id), 
             size = 0.5, alpha = 1) +
  xlab('Longitude') +
  ylab('Latitude') +
  ggtitle('Place Holder')
```

Just by visual inspection on the map, it's hard to see any patterns. What if we inspect the statistics within each clusters?

```{r}
kproto_results <- data %>%
  dplyr::select(-X,-occurrencedate,-reporteddate,-occurrencetime,-reportedtime, -Lat, -Long) %>%
  group_by(cluster_id) %>%
  do(the_summary = summary(.))
print(kproto_results$the_summary)
```

Again, it's not entirely obvious how each cluster differs from one another. However, we do spot an interesting observation on the 2nd cluster. Previously we observed that people can report crimes that happened much earlier than the time when they reported. The 2nd clusters seem to identify these reports (notice the difference of occurrenceyear and reportedyear in this clusters).

## VI. Evaluation

Throughout the clustering analysis in part V, we have evaluate the clustering quality by ploting visual representations of the clustering and by ploting the silhouette plot. As a short summary, using police-division-based aggregation and neighborhood-based aggregation both produced clustering results where we can spot a pattern in the clustering. Yet police-division-based aggregation seems to provide a higher quality of clustering. The other two clustering strategies that we have used, on the other hand, are not so informative, at least not from what we can tell immediately. 

## VII. Deployment

An R Shiny dashboard was created and [uploaded to shinyapps](https://tallmachinelearning.shinyapps.io/TorontoCrime/). We have produced this very informative Rshiny app which would be very interesting to those who is eager to know the crime levels and statistics in Toronto, for example, those who are looking to buy or rent properties. The app itself can be readily deployed to interested readers without much concerns regarding scaling since the dataset collected (or crime reported) are relatively limited in size unles richer dimensions are introduced to the data collecting activities.

The clusters themselves can be used for sending police to high crime areas. As more crime happens, the clusters can be updated to better understand where crime is happening in Toronto, and to see if increased police presence in certain areas helps to reduce crime. 

As with other data products, maintenance is expected to some extent to keep the data and analysis updated. The following steps can be taken:
(1) Collect data on an on-going basis;   
(2) Collect more features and experiment if additional features could further drive up clustering quality, or prediction accuracy in case that the clustering is used as a preprocessing step for predictive analysis;   
(3) Establish a way to evaluate the source of data inconsistencies during data collection activities;   
(4) Rerun modeling in a timely manner.

However, we don't need to worry about scaling.

## VIII. Conclusions

From what we understand from the data, the clustering activities are best served as:
(1) A preprocessing step for predictive analysis, which reduces the dimensions of the data and alleviates the problem of "the curse of dimensionality";
(2) Exploratory activities to help us understand the data. 

For this specific task on Toronto's crime statistics, one can readily recognize clusters based on how many offenses happened in each neighborhood or in each police division. Simple clustering algorithms such as k-means or hierarchical clustering help with recognizing and visualizing such clusters. They are an interesting way to visualize crime that happened in Toronto. Using this historical data, the Toronto Police Service can better allocate their resources to areas that have more crime, whether by neighbourhood, division, or natural hotspots. 

Some areas for improvement include looking at crime incidents for neighbourhood or division based on the population of the area. An area with a higher population might have higher crime incidents simply by virtue of having more residents. However, it would be interesting to know if an area had low crime but high population. Another aspect for improvement would be to look at the demographics of the areas. This would enable us to see if there is any bias in how crime occurrences are reported, and the data could be adjusted for this bias.