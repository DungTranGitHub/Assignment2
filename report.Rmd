---
title: "Toronto Major Crime Indicators - Clustering"
author: "TALL Machine Learning - Iman Lau, Dung Tran, Zheng (James) Lai"
date: "October 12, 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r warning=FALSE,message=FALSE}
library(ggplot2)
library(dplyr)
library(ggmap)
library(tidyr)
library(dplyr)
library(cluster)
library(lubridate)
library(rgl)
library(maptools)
library("RColorBrewer")
library(ggpubr)
library(viridis)
library(tidyverse)
model_dir = "models"
data_dir = "data"
#saved_models = list.files(model_dir)
#for(file in saved_models) {
#  load(paste(model_dir,file,sep="/"))
#}
data=read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
```

## I. Introduction

The second assignment for the York Machine Learning course, *Machine Learning in Business Context* was to explore unsupervised machine learning algorithms, specifically clustering. We chose a dataset from the Toronto Police Sercive Public Safety Data Portal, [MCI 2014 to 2017](http://data.torontopolice.on.ca/datasets/mci-2014-to-2017). This report follows the structures laid out in CRISP-DM methodology.

The GitHub repository for all the source code is located at the following link: [link here](link here).

The RShiny app is located at the following link: [link here](link here).

## II. Business Understanding

The [Toronto Police Service](https://en.wikipedia.org/wiki/Toronto_Police_Service) is the police force that serves the Greater Toronto Area. It is the largest municipal police force in Canada, and the third largest police force in Canada. They are a taxpayer funded service, ranking as second for the government of Toronto's budgetary expenses. There is always 

THe objective of this model is to cluster crimes to determine which areas of Toronto have the most levels of crime, overall and for different Major Crime Indicators. This would enable the Toronto Police to most effectively allocate their officers and specialists to the areas that require them the most. The hope is that this would be an effective way to lower crime rates and enable more cases to be solved, all without spending more money.

There are some ethical implications of using crime data. There are many avenues for bias to enter the data set. Police services around North America have come under increased scrutiny in recent years for racist policing. Some police policies or laws inherently disadvantage certain groups of people, which would create bias in the data. This means that conclusions drawn from a machine learning model based on biased data would create biased results. The conclusions should be looked at with other data, such as demographic data, and supplementary information, such as social considerations.

## III. Data Understanding

The data set was provided courtesy of the [Toronto Police Service Open Data Portal](http://data.torontopolice.on.ca/). It is usable in accorance with the [Open Government License - Ontario](https://www.ontario.ca/page/open-government-licence-ontario).

The data concerns all Major Crime Indicators (MCI) occurences in Toronto by reported date, between 2014 and 2017. The MCIs are Assault, Break and Enter, Auto Theft, and Theft Over (excludes Sexual Assaults). Locations in the data set are approximate, as they have been deliberately offset to the nearest road intersection to protect the privacy of involved individuals. 

There are 29 columns with 131,073 observations. However, 4 of these columns are duplicates, bringing us to 25 columns. Most of the columns

There are both numeric and categorical attributes, and are further divided by date-type data and crime-type data.

The following table shows the numeric attributes:


Attribute    | Description
-------------|------------------------------------------------------------------------
...          | ...

While this table shows the categorical attributes:

Attribute    | Description
-------------|------------------------------------------------------------------------
...          | ...

## IV. Data Exploration and Preparation

### (1) Visualizations for date-related attributes

### (2) Visualizations for the crime-related attributes

Visualization crime level for all neibourhoods.
```{r}
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% summarise(offence_cnt = n())
hood_total_offence_cnt_table = merge(total_offence_cnt_table,sh@data,by.x='Hood_ID',by.y='AREA_S_CD')
points_offense_cnt <- fortify(sh, region = 'AREA_S_CD')
points_offense_cnt <- merge(points_offense_cnt, hood_total_offence_cnt_table, by.x='id', by.y='Hood_ID', all.x=TRUE)
toronto <- qmap("Toronto, Ontario", zoom=10)
toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points_offense_cnt, color='black') +
  scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
```

### (3) Data outliers
It seems like someone reported crimes over 40 years ago.
```{r}
data$occurrencedate <- ymd(gsub("(.*)T.*", "\\1", data$occurrencedate))
data$reporteddate <- ymd(gsub("(.*)T.*", "\\1", data$reporteddate))
data[which(data$occurrencedate < as.POSIXct("1970-01-01")),]
```

### (4) Data correlations

### (5) Possible inconsistencies in data

### (6) Data Preprocessing

The following code documents the preprocessing done on the dataset, based on the data exploration done in this section.

```{r warning=FALSE,message=FALSE}
#reload the data
data <- read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
```

First we want to remove any duplicate data - rows or columns. Some events have duplicated event IDs and should be removed. We also have duplicate columns for X/Y and Lat/Long, which should be removed. We are don't use the UCR codes or the ID numbers, so they're also removed.

```{r warning=FALSE,message=FALSE}
#remove duplicate rows/entries
data <- subset(data, !duplicated(data$event_unique_id))

#remove columns that aren't used/duplicates
data <- data[, !colnames(data) %in% c("?..X","Y","Index_","event_unique_id","ucr_code","ucr_ext","FID")]

```

Next we format the dates. There are garbage time values at the end of the dates, which are removed. The other date values are also changed into appropriate date/time values. Whitespace is also present in the day of week columns, so that is trimmed.

```{r warning=FALSE,message=FALSE}
#formatting dates - remove garbage time values at the end
data$occurrencedate <- gsub("(.*)T.*", "\\1", data$occurrencedate)
data$reporteddate <- gsub("(.*)T.*", "\\1", data$reporteddate)
data$occurrencetime = ymd_h(paste(data$occurrencedate,data$occurrencehour,sep = " "), tz="EST")
data$reportedtime = ymd_h(paste(data$reporteddate,data$reportedhour,sep = " "), tz="EST")
data$occurrencedate = ymd(data$occurrencedate)
data$reporteddate = ymd(data$reporteddate)

#removing whitespace from day of week
data$occurrencedayofweek <- as.factor(trimws(data$occurrencedayofweek, "b"))
data$reporteddayofweek <- as.factor(trimws(data$reporteddayofweek, "b"))
```

Now let's take a look at the missing data:

```{r warning=FALSE,message=FALSE}
#missing data
colSums(is.na(data))
NAdata <- unique (unlist (lapply (data, function (x) which (is.na (x)))))
```
Rows with NA data:
```{r warning=FALSE,message=FALSE}
NAdata
```

Occurence dates for rows with NA data:
```{r warning=FALSE,message=FALSE}
data$occurrencedate[NAdata]
```

We can see that there are 32 missing values, all in occurence date type columns. Upon further inspection, these are all the same rows. We can also see that the occurrence date value is correct, so these date type columns can have their missing values imputed. 

```{r warning=FALSE,message=FALSE}
#imputing occurence dates from occurence date field
data$occurrenceyear[NAdata] <- year(data$occurrencedate[NAdata])
data$occurrencemonth[NAdata] <- month(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrenceday[NAdata] <- day(data$occurrencedate[NAdata])
data$occurrencedayofweek[NAdata] <- wday(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrencedayofyear[NAdata] <- yday(data$occurrencedate[NAdata])
```

Now we replace the space in the strings with an underscore for easier processing:

```{r warning=FALSE,message=FALSE}
#replace space in string
data$offence <- gsub("\\s", "_", data$offence)
data$MCI <- gsub("\\s", "_", data$MCI)
```

Next, all columns are converted into factors, with dates as ordered factors. Unused factor levels are also dropped (resulted from missing values).

```{r warning=FALSE,message=FALSE}

#change things to factors
for(col in c("offence","MCI","Division","Hood_ID")) {
  data[,col] = as.factor(data[,col])
}

#change things to ordered factors, useful for daisy() later
for(col in c("reportedyear","reportedmonth","reportedday","reporteddayofyear","reporteddayofweek",
             "reportedhour","occurrenceyear","occurrencemonth","occurrenceday","occurrencedayofyear",
             "occurrencedayofweek","occurrencehour")) {
  data[,col] = ordered(data[,col])
}

#drop unused factor levels
for(col in names(data)) {
  if(is.factor(data[,col])) {
    data[,col] = droplevels(data[,col])
  }
}
```

Finally, we cherck for missing data one last time:

```{r warning=FALSE,message=FALSE}
# Check missing values one last time
colSums(is.na(data))
```

## V. Modeling

With the data prepared, we can now start looking at models. 

### (1) Clustering strategy I.

First we can look at clustering crime by neighbourhood. We need to coerce the data into a clusterable table, sorted by MCI and neighbourhood.

```{r}
# Neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Hood_ID)
groups <- summarise(bygroup, n=n())
groups <- groups[c("Hood_ID", "MCI", "n")]
hood <- as.data.frame(spread(groups, key=MCI, value=n))
hood_id = as.integer(hood[,"Hood_ID"])
hood = hood[,-1]
head(hood)
```

Then we can use this table to perform k-means clustering. First we need to normalize the data and determine the number of clusters.

```{r}
#normalize data
for(col in names(hood)) {
  hood[,col] = (hood[,col] - mean(hood[,col])) / sd(hood[,col])
}

#determine number of clusters
wssplot <- function(data, nc=15, seed=1234) {
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc) {
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)
    }
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}

#we can see there's an elbow around 3 clusters
wssplot(hood, nc=15)
```

There is an elbow around 3 clusters, which appears to be a good number of clusters.

```{r}
# k-means
k.means.fit <- kmeans(hood, 3)
k.means.fit
```

From the above k-means cluster result, we can see that cluster 3 has lower than average crime. It has 93 neighbourhoods, meaning that the majority of Toronto is quite safe in terms of number of incidents occuring. Cluster 1 has slightly above average crime numbers, and it only has 37 neighbourhoods. 10 neighbourhoods have much higher than average crime numbers, which is seen in cluster 2, meaning that crime is concentrated in these small pockets of Toronto.

```{r}

#plotting k-means
clusplot(hood, k.means.fit$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)

#we can see there is some overlap between cluster 1 and 3, which means they could potentially be one cluster
#implies that they are more similar to each other than they are to cluster 2
```

If we plot a 2D representation of the cluster, we can see that clusters 1 and 3 are tightly packed and overlap slightly. This means that they are quite simnilar to each other. Cluster 2 on the other had, is more broadly spread, meaning those neighbourhoods are more dissimilar to one another.

Looking at a 3D representation, we can see how close clusters 1 and 3 are. However, with a third dimension, we can see that there is more of a spread than initially can be seen simply in two dimensions.

```{r}
#3D plot
pc <-princomp(hood, cor=TRUE, scores=TRUE)
plot3d(pc$scores[,1:3], col=k.means.fit$cluster, main="k-means clusters")
```

Next we can look at the neighbourhoods from a heirarchichal clustering approach. We calculate the distance and then can plot the dendrogram, with the clusters shown with a red border.
```{r}
#hierarchical
d <- dist(hood, method = "euclidean") # Euclidean distance matrix.
H.fit <- hclust(d, method="ward.D2")

#plot dendrogram
plot(H.fit) # display dendogram
cluster_ids <- cutree(H.fit, k=3) # cut tree into 3 clusters
# draw dendogram with red borders around the 3 clusters
rect.hclust(H.fit, k=3, border="red") 
```

If we want to look at whether or not more or less clusters would be appropriate:

```{r}
# if we want to look at other numbers of clusters
counts <- sapply(2:6, function(ncl) table(cutree(H.fit, ncl)))
names(counts) <- 2:6
counts
```

We can see that 3 clusters is ideal. We don't want to split the neighbourhoods into clusters with a small number of neighbourhoods.

Now plot grouping on map
```{r}
hood_ids_and_cluster_ids <- data.frame(cbind(hood_id,cluster_ids))
hood_ids_and_cluster_ids$cluster_ids = as.factor(hood_ids_and_cluster_ids$cluster_ids)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)

hood_name_and_cluster_ids = merge(hood_ids_and_cluster_ids,sh@data,by.x='hood_id',by.y='AREA_S_CD')
points_clustering <- fortify(sh, region = 'AREA_S_CD')
points_clustering <- merge(points_clustering, hood_name_and_cluster_ids, by.x='id', by.y='hood_id', all.x=TRUE)
toronto <- qmap("Toronto, Ontario", zoom=10)
# toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points2, color='black') +
  #scale_fill_gradient(low='white', high='red')

p1 = toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=cluster_ids), data=points_clustering, color='black') +
  scale_fill_manual(values = c("red", "green", "blue"))
p2 = toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points_offense_cnt, color='black') +
  scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
p3 = ggarrange(p1, p2, ncol = 2, nrow = 1, common.legend = F)
print(p3)
```

### (2) Clustering strategy II.

We can perform clustering on lat/long to look at natural crime hotspots. We can compare this to a manual cluster like the Toronto police divisions.

```{r}
latlong <- data[, colnames(data) %in% c("Lat", "Long")]

# k-means 
# 34 divisions in Toronto
k.means.fit <- kmeans(latlong, 34)
str(k.means.fit)

torclus <- as.data.frame(k.means.fit$centers)
torclus$size <- k.means.fit$size

latlongclus <- latlong
latlongclus$cluster <- as.factor(k.means.fit$cluster)

tormap <- get_map(location =c(left=-79.8129, bottom=43.4544, right=-78.9011, top=43.9132))

#plot each incident, colour coded by cluster
ggmap(tormap) +
  geom_point( data= latlongclus, aes(x=Long[], y=Lat[], color= as.factor(cluster))) +
  theme_void() + coord_map() 

#plot bubble map with cluster centroids, size/colour determined by number of incidents in each cluster
ggmap(tormap) +
  geom_point( data= torclus, aes(x=Long[], y=Lat[], size=size, color=size)) +
  theme_void() + coord_map() 
```

We can see how the clusters look with both the incidents plotted as well as the centroids. Next, we can perform k-means clustering on the natural crime hotspot clusters and on the Toronto Police divisions to see how they compare to one another.

```{r}

#coerce data for Hotspots
data2 <- data
data2$cluster <- k.means.fit$cluster
bygroup <- group_by(data2, MCI, cluster)
groups <- summarise(bygroup, n=n())
groups <- groups[c("cluster", "MCI", "n")]
hotspot <- as.data.frame(spread(groups, key=MCI, value=n))
hotspot <- hotspot[, -1]

#normalize
for(col in names(hotspot)) {
  hotspot[,col] = (hotspot[,col] - mean(hotspot[,col])) / sd(hotspot[,col])
}

#determine number of clusters
#we can see there's an elbow around 4 clusters
wssplot(hotspot, nc=15)

# k-means
k.means.fit <- kmeans(hotspot, 4)
k.means.fit
```

We can see here that there is an elbow around 4, so we can run k means with 4 clusters. With these 4 clusters, we can see that cluster 1 has higher than average auto thefts, and slightly higher roberies, but is below average for the other 3 MCIs. Cluster 2 is a safe part of Toronto with lower crime incidents. Cluster 3 has slightly more assaults, break and enters, and robberies, but lower auto thefts and theft overs. Cluster 4 however is the most crime-centric area of Toronto, with many more incidents than average, excepting auto thefts. Luckily, there are only 3 hotspots in cluster 4, while there are 12 and 13 in clusters 2 and 3 respectively, which had generally fewer incidents overall. 

```{r}

#plotting k-means
clusplot(hotspot, k.means.fit$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)

pc <-princomp(hotspot, cor=TRUE, scores=TRUE)
plot3d(pc$scores[,1:3], col=k.means.fit$cluster, main="k-means clusters")

hotspot$cluster <- k.means.fit$cluster

ggmap(tormap) +
  geom_point( data= torclus, aes(x=Long[], y=Lat[], color= as.factor(hotspot$cluster))) +
  theme_void() + coord_map()

```

When we plot the clusters using principal component analysis in 2D, we can see that the clusters are relatively well separated. Cluster 1 and 4 are spread out, but clusters 2 and 3 are tighter. On the map, we can also see that cluster 4 is concentrated in downtown Toronto, while cluster 1 is in the northwest part. 

```{r}

#coerce data for Divisions
bygroup <- group_by(data, MCI, Division)
groups <- summarise(bygroup, n=n())
groups <- groups[c("Division", "MCI", "n")]
div <- as.data.frame(spread(groups, key=MCI, value=n))
div <- div[, -1]

#normalize
for(col in names(div)) {
  div[,col] = (div[,col] - mean(div[,col])) / sd(div[,col])
}

#determine number of clusters
#we can see there's an elbow around 3 clusters
wssplot(div, nc=15)

# k-means
k.means.fit <- kmeans(div, 3)
k.means.fit

#similar to neighbourhoods, two clusters have low crime incidents (1 and 2), while cluster 3 has higher crime incidents
# most districts are lower crime incident districts, while 9 specifically are higher
#can we map this and see if they correspond to neighbourhoods?
#do the toronto police have a dedicated district to each high crime neighbourhood?

#plotting k-means
clusplot(div, k.means.fit$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)

pc <-princomp(div, cor=TRUE, scores=TRUE)
plot3d(pc$scores[,1:3], col=k.means.fit$cluster, main="k-means clusters")
```

Can we map this?


### (3) Clustering strategy III.

Clustering on all events
```{r}
library(clustMixType)
# data_for_gower = data[,!names(data) %in% c("occurrencedate","reporteddate","reportedyear",
#                                           "reportedmonth","reportedday","reporteddayofyear",
#                                           "reporteddayofweek","reportedhour",
#                                           "Lat","Long","Hood_ID","Neighbourhood","Division")]
data_fullclust = data[,c("occurrencetime","reportedtime","premisetype","offence")]
data_fullclust$occurrencetime = as.numeric(data_fullclust$occurrencetime)
data_fullclust$reportedtime = as.numeric(data_fullclust$reportedtime)
# Are there skewed data?
kproto_cluster <- kproto(data_fullclust,5)
```

But how many clusters are enough?
```{r}
library(rlist)
set.seed(1234)
kproto_clusters = list()
for (i in seq(2,20,2)) {
  kproto_clusters = list.append(kproto_clusters,kproto(data_fullclust,i,lambda = 1))
}
```

```{r}
wss = c()
for(kc in kproto_clusters) {
  wss = c(wss,kc$tot.withinss)
}
plot(wss, type="b", xlab="Number of Clusters/2",ylab="Within groups sum of squares")
```

```{r}
kproto_selection = kproto_clusters[[3]]
data$cluster_id = as.factor(kproto_selection$cluster)
```

```{r}
to_map <- data.frame(data$MCI, data$Lat, data$Long, data$cluster_id)
colnames(to_map) <- c('crimes', 'lat', 'lon', 'cluster_id')
sbbox <- make_bbox(lon = data$Long, lat = data$Lat, f = 0.05)
my_map <- get_map(location = sbbox, maptype = "roadmap", scale = 2, color="color", zoom = 10)
ggmap(my_map) +
  geom_point(data=to_map, aes(x = lon, y = lat, color = cluster_id), 
             size = 0.01, alpha = 1) +
  xlab('Longitude') +
  ylab('Latitude') +
  ggtitle('Place Holder')
```

```{r}
kproto_results <- data %>%
  dplyr::select(-occurrencedate,-reporteddate,-occurrencetime,-reportedtime) %>%
  group_by(cluster_id) %>%
  do(the_summary = summary(.))
print(kproto_results$the_summary)
```


## VI. Evaluation

## VII. Deployment

## VIII. Conclusions