---
title: "Toronto Major Crime Indicators - Clustering"
author: "TALL Machine Learning - Iman Lau, Dung Tran, Zheng (James) Lai"
date: "October 12, 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r warning=FALSE,message=FALSE}
library(ggplot2)
library(dplyr)
library(ggmap)
library(tidyr)
library(dplyr)
library(cluster)
library(lubridate)
library(rgl)
library(maptools)
library("RColorBrewer")
library(ggpubr)
library(viridis)
library(tidyverse)
library(scatterplot3d)
library(factoextra)
library(fpc)
library(NbClust)
model_dir = "models"
data_dir = "data"
#saved_models = list.files(model_dir)
#for(file in saved_models) {
#  load(paste(model_dir,file,sep="/"))
#}
data=read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
```

## I. Introduction

The second assignment for the York Machine Learning course, *Machine Learning in Business Context* was to explore unsupervised machine learning algorithms, specifically clustering. We chose a dataset from the Toronto Police Sercive Public Safety Data Portal, [MCI 2014 to 2017](http://data.torontopolice.on.ca/datasets/mci-2014-to-2017). This report follows the structures laid out in CRISP-DM methodology.

The GitHub repository for all the source code is located at the following link: [link here](link here).

The RShiny app is located at the following link: [link here](link here).

## II. Business Understanding

The [Toronto Police Service](https://en.wikipedia.org/wiki/Toronto_Police_Service) is the police force that serves the Greater Toronto Area. It is the largest municipal police force in Canada and the third largest police force in Canada. They are a taxpayer-funded service, ranking as second for the government of Toronto's budgetary expenses. There is always 

The objective of this model is to cluster crimes to determine which areas of Toronto have the most levels of crime, overall and for different Major Crime Indicators. This would enable the Toronto Police to most effectively allocate their officers and specialists to the areas that require them the most. The hope is that this would be an effective way to lower crime rates and enable more cases to be solved, all without spending more money.

There are some ethical implications of using crime data. There are many avenues for bias to enter the data set. Police services around North America have come under increased scrutiny in recent years for racist policing. Some police policies or laws inherently disadvantage certain groups of people, which would create bias in the data. This means that conclusions drawn from a machine learning model based on biased data would create biased results. The conclusions should be looked at with other data, such as demographic data, and supplementary information, such as social considerations.

## III. Data Understanding

The data set was provided courtesy of the [Toronto Police Service Open Data Portal](http://data.torontopolice.on.ca/). It is usable in accordance with the [Open Government License - Ontario](https://www.ontario.ca/page/open-government-licence-ontario).

The data concerns all Major Crime Indicators (MCI) occurrences in Toronto by reported date, between 2014 and 2017. The MCIs are Assault, Break and Enter, Auto Theft, and Theft Over (excludes Sexual Assaults). Locations in the data set are approximate, as they have been deliberately offset to the nearest road intersection to protect the privacy of involved individuals. 

There are 29 columns with 131,073 observations. However, 4 of these columns are duplicates, bringing us to 25 columns. Most of the columns

There are both numeric and categorical attributes and are further divided by date-type data and crime-type data.

The following table shows the numeric attributes:


Attribute    | Description
-------------|------------------------------------------------------------------------
...          | ...

While this table shows the categorical attributes:

Attribute    | Description
-------------|------------------------------------------------------------------------
...          | ...

## IV. Data Exploration and Preparation

### (1) Visualizations for date-related attributes

### (2) Visualizations for the crime-related attributes

Visualization crime level for all neibourhoods.
```{r}
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% dplyr::summarise(offence_cnt = n())
hood_total_offence_cnt_table = merge(total_offence_cnt_table,sh@data,by.x='Hood_ID',by.y='AREA_S_CD')
points_offense_cnt <- fortify(sh, region = 'AREA_S_CD')
points_offense_cnt <- merge(points_offense_cnt, hood_total_offence_cnt_table, by.x='id', by.y='Hood_ID', all.x=TRUE)
torontoMap <- qmap("Toronto, Ontario", zoom=10)
save(torontoMap, file="map/toronto_map.RData")
torontoMap + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points_offense_cnt, color='black') +
  scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
```

### (3) Data outliers
It seems like someone reported crimes over 40 years ago. But since nothing stops people from doing this, we are not going to treat these as outliers.
```{r}
data$occurrencedate <- ymd(gsub("(.*)T.*", "\\1", data$occurrencedate))
data$reporteddate <- ymd(gsub("(.*)T.*", "\\1", data$reporteddate))
data[which(data$occurrencedate < as.POSIXct("1970-01-01")),]
```
The rest of the data is mostly categorical in their nature, so no concerns need to be addressed in terms of data outliers here.

### (4) Data Preprocessing
The following code documents the preprocessing steps idendified for this dataset.
```{r warning=FALSE,message=FALSE}
#reload the data
data <- read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
```

First, we want to remove any duplicate data - rows or columns. Some events have duplicated event IDs and should be removed. We also have duplicate columns for X/Y and Lat/Long, which should be removed. We are don't use the UCR codes or the ID numbers, so they're also removed.

```{r warning=FALSE,message=FALSE}
#remove duplicate rows/entries
data <- subset(data, !duplicated(data$event_unique_id))

#remove columns that aren't used/duplicates
data <- data[, !colnames(data) %in% c("?..X","Y","Index_","event_unique_id","ucr_code","ucr_ext","FID")]

```

Next, we format the dates. There are garbage time values at the end of the dates, which are removed. The other date values are also changed into appropriate date/time values. Whitespace is also present in the day of week columns, so that is trimmed.

```{r warning=FALSE,message=FALSE}
#formatting dates - remove garbage time values at the end
data$occurrencedate <- gsub("(.*)T.*", "\\1", data$occurrencedate)
data$reporteddate <- gsub("(.*)T.*", "\\1", data$reporteddate)
data$occurrencetime = ymd_h(paste(data$occurrencedate,data$occurrencehour,sep = " "), tz="EST")
data$reportedtime = ymd_h(paste(data$reporteddate,data$reportedhour,sep = " "), tz="EST")
data$occurrencedate = ymd(data$occurrencedate)
data$reporteddate = ymd(data$reporteddate)

#removing whitespace from day of week
data$occurrencedayofweek <- as.factor(trimws(data$occurrencedayofweek, "b"))
data$reporteddayofweek <- as.factor(trimws(data$reporteddayofweek, "b"))
```

We also convert the month/day of week from string representation to integer representation:
```{r}
data$reportedmonth = month(data$reportedtime)
data$reporteddayofweek = wday(data$reportedtime)
data$occurrencemonth = month(data$occurrencetime)
data$occurrencedayofweek = wday(data$occurrencetime)
```

Now let's take a look at the missing data:

```{r warning=FALSE,message=FALSE}
#missing data
colSums(is.na(data))
NAdata <- unique (unlist (lapply (data, function (x) which (is.na (x)))))
```
Rows with NA data:
```{r warning=FALSE,message=FALSE}
NAdata
```

Occurence dates for rows with NA data:
```{r warning=FALSE,message=FALSE}
data$occurrencedate[NAdata]
```

We can see that there are 32 missing values, all in occurrence date related columns. Upon further inspection, these are all the same rows. We can also see that the occurrence date value is correct, so these date type columns can have their missing values imputed. 

```{r warning=FALSE,message=FALSE}
#imputing occurence dates from occurence date field
data$occurrenceyear[NAdata] <- year(data$occurrencedate[NAdata])
data$occurrencemonth[NAdata] <- month(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrenceday[NAdata] <- day(data$occurrencedate[NAdata])
data$occurrencedayofweek[NAdata] <- wday(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrencedayofyear[NAdata] <- yday(data$occurrencedate[NAdata])
```

Now we replace the space in the strings with an underscore for easier processing:

```{r warning=FALSE,message=FALSE}
#replace space in string
data$offence <- gsub("\\s", "_", data$offence)
data$MCI <- gsub("\\s", "_", data$MCI)
```

Next, all columns are converted into factors except Lat, Long, reportedtime, and occurrencetime. Unused factor levels are also dropped (resulted from missing values).

```{r warning=FALSE,message=FALSE}

#change things to factors
for(col in c("offence","MCI","Division","Hood_ID")) {
  data[,col] = as.factor(data[,col])
}

#if we use the gower distance and daisy() function, the following metrics can be considered to converted to ordered factor; but since gower distance turns out to be too expensive for large dataset, we have treated the following as normal factors as well!
for(col in c("reportedyear","reportedmonth","reportedday","reporteddayofyear","reporteddayofweek",
             "reportedhour","occurrenceyear","occurrencemonth","occurrenceday","occurrencedayofyear",
             "occurrencedayofweek","occurrencehour")) {
  data[,col] = factor(data[,col])
}

#drop unused factor levels
for(col in names(data)) {
  if(is.factor(data[,col])) {
    data[,col] = droplevels(data[,col])
  }
}
```

Finally, we cherck for missing data one last time:

```{r warning=FALSE,message=FALSE}
# Check missing values one last time
colSums(is.na(data))
```

## V. Modeling

With the data prepared, we can now start looking at models. 

### (1) Clustering strategy I.
First, we can look at clustering crime by neighborhood. We need to coerce the data into a clusterable table, sorted by MCI and neighborhood.

```{r}
# Neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Hood_ID)
groups <- dplyr::summarise(bygroup, n=n())
groups <- groups[c("Hood_ID", "MCI", "n")]
hood <- as.data.frame(spread(groups, key=MCI, value=n))
hood_id = as.integer(hood[,"Hood_ID"])
hood = hood[,-1]
head(hood)
```

Then we can use this table to perform k-means clustering. First, we need to normalize the data and determine the number of clusters. To determine the number of clusters, we simply plot the within-cluster sum-of-squares and pick a number after inspecting this plot.
```{r}
hood <- scale(hood)
#determine number of clusters
wssplot <- function(data, nc=15, seed=1234) {
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc) {
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)
    }
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}

#we can see there's an elbow around 3 clusters
wssplot(hood, nc=15)
```
Now it seems number 3 is a good choice of the number of clusters. We then proceed to perform K-means clustering on the data-set. We have followed this tutorial(https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/) to evaluate the quality of clustering results. Specifically, we have used Silhouette width as a quantitative measures to evaluate the clustering quality. As we will see in the Silhouette plot, both in cluster #2 and cluster #3 have data points where the Silhouette width falls to negative, indicating a not so ideal clustering. This can also be observed in the 1st plot where we see that cluster #2 and cluster #3 is quite close to each other.
```{r}
# K-means clustering
km.res <- eclust(hood, "kmeans", k = 3, graph = T)
fviz_silhouette(km.res, palette = "jco", ggtheme = theme_classic())
```

Let's look at the cluster means from the outputs to see what we can learn.
```{r}
# k-means results
km.res
```

We can see that cluster 3 has lower than average crime. It has 93 neighborhoods, meaning that the majority of Toronto is quite safe in terms of the number of incidents occurring. Cluster 2 has slightly above average crime numbers, and it only has 37 neighborhoods. 10 neighborhoods have much higher than average crime numbers, which is seen in cluster 3. In some way, we can interpret the results as such that crime is concentrated in these small pockets of Toronto.

As an interesting activity, we also looking at a 3D representation of the clustering results. With 3 principal components, more variations should be captured that as in the case in 2 principal components. With a third dimension, we can see that there is more of a spread than initially can be seen simply in two dimensions.
```{r}
#3D plot
pc <-princomp(hood, cor=TRUE, scores=TRUE)
plot3d(pc$scores[,1:3], col=km.res$cluster, main="k-means clusters")
```

Next, we can look at the neighborhoods from a hierarchical clustering approach. Again, we need to determine how many clusters we want to have. For hierarchical clustering, we look at the total number of observations that end up in different clusters with different configurations(in this case, the configuration is the number of clusters).
```{r}
counts <- sapply(2:6, function(ncl) eclust(hood, "hclust", k = ncl, hc_metric = "euclidean", hc_method = "ward.D2")$size)
names(counts) <- 2:6
counts
```
Still, we can see that 3 clusters are not bad if we consider only the cluster size as the criteria for choosing the number of clusters. We don't want to split the neighborhoods into clusters with a small number of neighborhoods.

Now that we have chosen the number of clusters, we proceed to the clustering process. As previously in k-means, we look at Silhouette width as a quantitative measure to evaluate the clustering quality. To visually inspect the clusters, we plot dendrograms.
```{r}
# Hierarchical clustering
hc.res <- eclust(hood, "hclust", k = 3, hc_metric = "euclidean", 
                 hc_method = "ward.D2")

# Visualize dendrograms
fviz_dend(hc.res, show_labels = FALSE,
         palette = "jco", as.ggplot = TRUE)

fviz_silhouette(hc.res, palette = "jco", ggtheme = theme_classic())
```
It seems like we have reached the same conlusion as the k-means algorithms: cluster #1 and cluster #2 does not have large gap. The following codes plot the clustering results on the map. We also plot the offence count in each neiborhood and it can be seen that the 2nd plot is highly correlated with the 1st plot (which makes sense).
```{r}
hood_ids_and_cluster_ids <- data.frame(cbind(hood_id,cluster_ids))
hood_ids_and_cluster_ids$cluster_ids = as.factor(hood_ids_and_cluster_ids$cluster_ids)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)

hood_name_and_cluster_ids = merge(hood_ids_and_cluster_ids,sh@data,by.x='hood_id',by.y='AREA_S_CD')
points_clustering <- fortify(sh, region = 'AREA_S_CD')
points_clustering <- merge(points_clustering, hood_name_and_cluster_ids, by.x='id', by.y='hood_id', all.x=TRUE)
toronto <- qmap("Toronto, Ontario", zoom=10)
# toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points2, color='black') +
  #scale_fill_gradient(low='white', high='red')

p1 = toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=cluster_ids), data=points_clustering, color='black') +
  scale_fill_manual(values = c("red", "green", "blue"))
p2 = toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points_offense_cnt, color='black') +
  scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
p3 = ggarrange(p1, p2, ncol = 2, nrow = 1, common.legend = F)
print(p3)
```

### (2) Clustering strategy II.
We can perform clustering on lat/long to look at natural crime hotspots. We can compare this to a manual cluster like the Toronto police divisions.

```{r}
latlong <- data[, colnames(data) %in% c("Lat", "Long")]

# k-means 
# 34 divisions in Toronto
k.means.fit <- kmeans(latlong, 34)
torclus <- as.data.frame(k.means.fit$centers)
torclus$size <- k.means.fit$size
latlongclus <- latlong
latlongclus$cluster <- as.factor(k.means.fit$cluster)
tormap <- get_map(location =c(left=-79.8129, bottom=43.4544, right=-78.9011, top=43.9132))

#plot each incident, color-coded by cluster
ggmap(tormap) +
  geom_point( data= latlongclus, aes(x=Long[], y=Lat[], color= as.factor(cluster))) +
  theme_void() + coord_map() 

#plot bubble map with cluster centroids, size/ color determined by the number of incidents in each cluster
ggmap(tormap) +
  geom_point( data= torclus, aes(x=Long[], y=Lat[], size=size)) +
  theme_void() + coord_map() 
```

We can see how the clusters look with both the incidents plotted as well as the centroids. Next, we can perform k-means clustering on the natural crime hotspot clusters and on the Toronto Police divisions to see how they compare to one another.
```{r}
#coerce data for Hotspots
data2 <- data
data2$cluster <- k.means.fit$cluster
bygroup <- group_by(data2, MCI, cluster)
groups <- dplyr::summarise(bygroup, n=n())
groups <- groups[c("cluster", "MCI", "n")]
hotspot <- data.frame(spread(groups, key=MCI, value=n))
hotspot <- hotspot[, -1]
hotspot = data.frame(scale(hotspot))
#determine number of clusters
#we can see there's an elbow around 4 clusters
wssplot(hotspot, nc=15)
```

We can see here that there is an elbow around 4, so we can run k means with 4 clusters. With these 4 clusters, we can see that cluster 4 has higher than average auto thefts, and slightly higher robberies, but is below average for the other 3 MCIs. Cluster 1 is a safe part of Toronto with lower crime incidents. Cluster 3 has slightly more assaults, break and enters, and robberies, but lower auto thefts and theft overs. Cluster 2, however, is the most crime-centric area of Toronto, with many more incidents than average, excepting auto thefts. Luckily, there are only 3 hotspots in cluster 2, while there are 12 and 13 in clusters 1 and 3 respectively, which had generally fewer incidents overall. 

From the silhouette plot, we can see that clustering based on this data set is better as compared to Clustering strategy I.
```{r}
km.res <- eclust(hotspot, "kmeans", k = 4, graph = T)
fviz_silhouette(km.res, palette = "jco", ggtheme = theme_classic())
km.res
```

```{r}
#plotting k-means
clusplot(hotspot, km.res$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)

#pc <-princomp(hotspot, cor=TRUE, scores=TRUE)
#plot3d(pc$scores[,1:3], col=k.means.fit$cluster, main="k-means clusters")

hotspot$cluster <- factor(km.res$cluster)

ggmap(tormap) +
  geom_point( data= torclus, aes(x=Long[], y=Lat[], color = hotspot$cluster)) +
  theme_void() + coord_map()
```

When we plot the clusters using principal component analysis in 2D, we can see that the clusters are relatively well separated. Cluster 1 and 4 are spread out, but clusters 2 and 3 are tighter. On the map, we can also see that cluster 4 is concentrated in downtown Toronto, while cluster 1 is in the northwest part. 

```{r}
#coerce data for Divisions
bygroup <- group_by(data, MCI, Division)
groups <- dplyr::summarise(bygroup, n=n())
groups <- groups[c("Division", "MCI", "n")]
div <- as.data.frame(spread(groups, key=MCI, value=n))
div_name = div[,1]
div <- div[, -1]

#normalize
for(col in names(div)) {
  div[,col] = (div[,col] - mean(div[,col])) / sd(div[,col])
}

#determine number of clusters
#we can see there's an elbow around 3 clusters
wssplot(div, nc=15)

# k-means
km.res <- eclust(div, "kmeans", k = 3, graph = T)
fviz_silhouette(km.res, palette = "jco", ggtheme = theme_classic())
km.res

#similar to neighborhoods, two clusters have low crime incidents (1 and 2), while cluster 3 has higher crime incidents
# most districts are lower crime incident districts, while 9 specifically are higher
#can we map this and see if they correspond to neighborhoods?
#do the Toronto police have a dedicated district to each high crime neighborhood?
```

Mapping this is probably not possible
```{r}
division_vs_hood_id = data[,c("Division","Hood_ID")]
table(division_vs_hood_id)
```


### (3) Clustering strategy III.
So far we have aggregated crime statistics into either neibourhood or division. What if we run clustering on data that excludes geographical information. Does such clustering produce naturally occuring crime zone? We will find out in the following analysis.
```{r}
library(clustMixType)
# data_for_gower = data[,!names(data) %in% c("occurrencedate","reporteddate","reportedyear",
#                                           "reportedmonth","reportedday","reporteddayofyear",
#                                           "reporteddayofweek","reportedhour",
#                                           "Lat","Long","Hood_ID","Neighbourhood","Division")]
data_fullclust = data[,c("occurrencetime","reportedtime","premisetype","offence","MCI")]
data_fullclust$occurrencetime = as.numeric(data_fullclust$occurrencetime)
data_fullclust$reportedtime = as.numeric(data_fullclust$reportedtime)
```

However, one important distinction as compared to previous clustering analysises is that, we now have mixed data type, being numeric and categorical. To handle this situation, we have experimented daisy and Gower's distance formula. However, Gower's distance formula drastically increase memory consumption when the dataset is large, making it not pratical to run on our own computers. As such, we swtiched to another algorithms "k-prototypes clustering" that can handle mixed type data. As in kmeans, we determine the number of clusters based on the trends shown in within cluster sum of squares.
```{r}
library(rlist)
set.seed(1234)
kproto_clusters = list()
for (i in seq(2,10,1)) {
  kproto_clusters = list.append(kproto_clusters,kproto(data_fullclust,i,lambda = 1))
}
```

```{r}
wss = c()
for(kc in kproto_clusters) {
  wss = c(wss,kc$tot.withinss)
}
plot(seq(2,10,1),wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
```
We see here that, beyond 4 clusters (or 7 clusters), there seems to be diminishing returns in reducing within-cluster sum of squres. We choose 4 clusters in this case. Then plot the clustering results on a map:
```{r}
kproto_selection = kproto_clusters[[3]]
data$cluster_id = as.factor(kproto_selection$cluster)
to_map <- data.frame(data$MCI, data$Lat, data$Long, data$cluster_id)
colnames(to_map) <- c('crimes', 'lat', 'lon', 'cluster_id')
sbbox <- make_bbox(lon = data$Long, lat = data$Lat, f = 0.05)
my_map <- get_map(location = sbbox, maptype = "roadmap", scale = 2, color="color", zoom = 10)
ggmap(my_map) +
  geom_point(data=to_map, aes(x = lon, y = lat, color = cluster_id), 
             size = 0.5, alpha = 1) +
  xlab('Longitude') +
  ylab('Latitude') +
  ggtitle('Place Holder')
```
Just by visual inspection on the map, it's hard to see any patterns. What if we inspect the statistics within each clusters?
```{r}
kproto_results <- data %>%
  dplyr::select(-X,-occurrencedate,-reporteddate,-occurrencetime,-reportedtime, -Lat, -Long) %>%
  group_by(cluster_id) %>%
  do(the_summary = summary(.))
print(kproto_results$the_summary)
```
Again, it's not entirely obvious how each cluster differs from one another. However, we do spot an interesting observation on the 2nd cluster. Previously we observed that people can report crimes that happened much earlier than the time when they reported. The 2nd clusters seem to identify these reports(notice the difference of occurrenceyear and reportedyear in this clusters).

## VI. Evaluation

## VII. Deployment

## VIII. Conclusions