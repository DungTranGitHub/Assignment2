---
title: "Toronto Major Crime Indicators - Clustering"
author: "TALL Machine Learning - Iman Lau, Dung Tran, Zheng (James) Lai"
date: "October 12, 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r warning=FALSE,message=FALSE}
library(ggplot2)
library(dplyr)
library(ggmap)
model_dir = "models"
data_dir = "data"
#saved_models = list.files(model_dir)
#for(file in saved_models) {
#  load(paste(model_dir,file,sep="/"))
#}
data=read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
```

## I. Introduction

The second assignment for the York Machine Learning course, *Machine Learning in Business Context* was to explore unsupervised machine learning algorithms, specifically clustering. We chose a dataset from the Toronto Police Sercive Public Safety Data Portal, [MCI 2014 to 2017](http://data.torontopolice.on.ca/datasets/mci-2014-to-2017). This report follows the structures laid out in CRISP-DM methodology.

The GitHub repository for all the source code is located at the following link: [link here](link here).

The RShiny app is located at the following link: [link here](link here).

## II. Business Understanding

The [Toronto Police Service](https://en.wikipedia.org/wiki/Toronto_Police_Service) is the police force that serves the Greater Toronto Area. It is the largest municipal police force in Canada, and the third largest police force in Canada. They are a taxpayer funded service, ranking as second for the government of Toronto's budgetary expenses. There is always 

THe objective of this model is to cluster crimes to determine which areas of Toronto have the most levels of crime, overall and for different Major Crime Indicators. This would enable the Toronto Police to most effectively allocate their officers and specialists to the areas that require them the most. The hope is that this would be an effective way to lower crime rates and enable more cases to be solved, all without spending more money.

There are some ethical implications of using crime data. There are many avenues for bias to enter the data set. Police services around North America have come under increased scrutiny in recent years for racist policing. Some police policies or laws inherently disadvantage certain groups of people, which would create bias in the data. This means that conclusions drawn from a machine learning model based on biased data would create biased results. The conclusions should be looked at with other data, such as demographic data, and supplementary information, such as social considerations.

## III. Data Understanding

The data set was provided courtesy of the [Toronto Police Service Open Data Portal](http://data.torontopolice.on.ca/). It is usable in accorance with the [Open Government License - Ontario](https://www.ontario.ca/page/open-government-licence-ontario).

The data concerns all Major Crime Indicators (MCI) occurences in Toronto by reported date, between 2014 and 2017. The MCIs are Assault, Break and Enter, Auto Theft, and Theft Over (excludes Sexual Assaults). Locations in the data set are approximate, as they have been deliberately offset to the nearest road intersection to protect the privacy of involved individuals. 

There are 29 columns with 131,073 observations. However, 4 of these columns are duplicates, bringing us to 25 columns. Most of the columns

There are both numeric and categorical attributes, and are further divided by client data, marketing details, and social/economic attributes.

The following table shows the numeric attributes:


Attribute    | Description
-------------|------------------------------------------------------------------------
...          | ...

While this table shows the categorical attributes:

Attribute    | Description
-------------|------------------------------------------------------------------------
...          | ...

## IV. Data Exploration and Preparation

### (1) Visualizations for client-related attributes

### (2) Visualizations for the social/economic attributes
Visualization crime level for all neibourhoods.
```{r}
library(maptools)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% summarise(offence_cnt = n())
hood_total_offence_cnt_table = merge(total_offence_cnt_table,sh@data,by.x='Hood_ID',by.y='AREA_S_CD')
points_offense_cnt <- fortify(sh, region = 'AREA_S_CD')
points_offense_cnt <- merge(points_offense_cnt, hood_total_offence_cnt_table, by.x='id', by.y='Hood_ID', all.x=TRUE)
toronto <- qmap("Toronto, Ontario", zoom=10)
library("RColorBrewer")
toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points_offense_cnt, color='black') +
  scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
```

### (3) Data outliers

### (4) Data correlations

### (5) Possible inconsistencies in data

### (6) Data Preprocessing

The following code documents the preprocessing done on the dataset, based on the data exploration done in this section.

```{r warning=FALSE,message=FALSE}
library(lubridate)
data_dir <- "data"
data <- read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
#remove any duplicate rows/entries
data <- subset(data, !duplicated(data$event_unique_id))

#remove columns that aren't useful/duplicates
#duplicates of other columns, UCR codes - not used in this case, ID number - not needed
data <- data[, !colnames(data) %in% c("X","Y","Index_","event_unique_id","ucr_code","ucr_ext","FID")]
# Keep "Hood ID" because we need this when we plot clustering results on a map

#formatting dates - remove garbage time values at the end
data$occurrencedate <- ymd(gsub("(.*)T.*", "\\1", data$occurrencedate))
data$reporteddate <- ymd(gsub("(.*)T.*", "\\1", data$reporteddate))

#removing whitespace from day of week
data$occurrencedayofweek <- as.factor(trimws(data$occurrencedayofweek, "b"))
data$reporteddayofweek <- as.factor(trimws(data$reporteddayofweek, "b"))

#missing data
#colSums(is.na(data))
NAdata <- unique (unlist (lapply (data, function (x) which (is.na (x)))))

#imputing occurence dates from occurence date field
data$occurrenceyear[NAdata] <- year(data$occurrencedate[NAdata])
data$occurrencemonth[NAdata] <- month(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrenceday[NAdata] <- day(data$occurrencedate[NAdata])
data$occurrencedayofweek[NAdata] <- wday(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrencedayofyear[NAdata] <- yday(data$occurrencedate[NAdata])

#change things to factors
for(col in c("reportedyear","reportedday","reporteddayofyear","reportedhour","occurrenceyear","occurrenceday",
             "occurrencedayofyear","occurrencehour")) {
  data[,col] = as.factor(data[,col])
}

#replace space in string
data$offence <- gsub("\\s", "_", data$offence)
data$MCI <- gsub("\\s", "_", data$MCI)

#drop unused factor levels
for(col in names(data)) {
  if(is.factor(data[,col])) {
    data[,col] = droplevels(data[,col])
  }
}

# Check missing values one last time
colSums(is.na(data))
```

## V. Modeling

With this preparation in mind, we can now start looking at models. 

### (1) Clustering strategy I.
clustering crime by police division and neighbourhood
```{r}
library(tidyr)
library(dplyr)
library(cluster)

# Neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Hood_ID)
groups <- summarise(bygroup, n=n())
groups <- groups[c("Hood_ID", "MCI", "n")]
hood <- as.data.frame(spread(groups, key=MCI, value=n))
hood_id = as.integer(hood[,"Hood_ID"])
hood = hood[,-1]
```

k-means
```{r}
#normalize data
for(col in names(hood)) {
  hood[,col] = (hood[,col] - mean(hood[,col])) / sd(hood[,col])
}

#determine number of clusters
wssplot <- function(data, nc=15, seed=1234) {
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc) {
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)
    }
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}

#we can see there's an elbow around 3 clusters
wssplot(hood, nc=15)

# k-means
k.means.fit <- kmeans(hood, 3)
k.means.fit

#cluster 3 has lower than average crime, while cluster 2 has higher than average. cluster 1 sits in the middle for incidents
#cluster 3 is also the one with the most number of neighbourhoods which implies that the majority of toronto is very safe
#there are only 10 neighbourhoods in cluster 2, which means crime is concentrated in these small pockets of toronto

#plotting k-means
clusplot(hood, k.means.fit$cluster, main='2D representation of the Cluster solution',
         color=TRUE, shade=TRUE,
         labels=2, lines=0)

#we can see there is some overlap between cluster 1 and 3, which means they could potentially be one cluster
#implies that they are more similar to each other than they are to cluster 2
```

hierarchical
```{r}
#hierarchical
d <- dist(hood, method = "euclidean") # Euclidean distance matrix.
H.fit <- hclust(d, method="ward.D2")

#plot dendrogram
plot(H.fit) # display dendogram
cluster_ids <- cutree(H.fit, k=3) # cut tree into 3 clusters
# draw dendogram with red borders around the 3 clusters
rect.hclust(H.fit, k=3, border="red") 

# if we want to look at other numbers of clusters
counts <- sapply(2:6, function(ncl) table(cutree(H.fit, ncl)))
names(counts) <- 2:6
counts

# we can see that 3 clusters is likely ideal because we don't want to split many clusters with small numbers of neighbourhoods
```

Now plot grouping on map
```{r}
hood_ids_and_cluster_ids <- data.frame(cbind(hood_id,cluster_ids))
hood_ids_and_cluster_ids$cluster_ids = as.factor(hood_ids_and_cluster_ids$cluster_ids)
library(maptools)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)

hood_name_and_cluster_ids = merge(hood_ids_and_cluster_ids,sh@data,by.x='hood_id',by.y='AREA_S_CD')
points_clustering <- fortify(sh, region = 'AREA_S_CD')
points_clustering <- merge(points_clustering, hood_name_and_cluster_ids, by.x='id', by.y='hood_id', all.x=TRUE)
toronto <- qmap("Toronto, Ontario", zoom=10)
# toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points2, color='black') +
  #scale_fill_gradient(low='white', high='red')

library("RColorBrewer")
library(ggpubr)
p1 = toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=cluster_ids), data=points_clustering, color='black') +
  scale_fill_manual(values = c("red", "green", "blue"))
p2 = toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points_offense_cnt, color='black') +
  scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
p3 = ggarrange(p1, p2, ncol = 2, nrow = 1, common.legend = F)
print(p3)
```


## VI. Evaluation

## VII. Deployment

## VIII. Conclusions
