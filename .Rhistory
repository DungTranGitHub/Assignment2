data <- data[, !colnames(data) %in% c("X","Y","Index_","event_unique_id","ucr_code","ucr","FID","Hood_ID")]
dim(data)
str(data)
#formatting dates - remove garbage time values at the end
data$occurrencedate <- ymd(gsub("(.*)T.*", "\\1", data$occurrencedate))
data$reporteddate <- ymd(gsub("(.*)T.*", "\\1", data$reporteddate))
head(data)
?ymd
?trimws
data$occurrencedayofweek
data$occurrencedayofweek[1]
data$occurrencedayofweek[2]
data$occurrencedayofweek[3]
levels(data$occurrencedayofweek)
levels(data$occurrencedayofweek)[1]
levels(data$occurrencedayofweek)[2]
levels(data$occurrencedayofweek)[3]
levels(data$occurrencedayofweek)[4]
levels(data$occurrencedayofweek)[6]
levels(data$occurrencedayofweek)[7]
levels(data$occurrencedayofweek)[8]
#removing whitespace from day of week
data$occurrencedayofweek <- as.factor(trimws(data$occurrencedayofweek, "b"))
data$reporteddayofweek <- as.factor(trimws(data$reporteddayofweek, "b"))
#missing data
colSums(is.na(data))
?unlist
is.na(data$occurrenceday)
is.na(data$occurrenceday)
which(is.na(data$occurrenceday))
NAdata
library(lubridate)
data_dir <- "data"
data <- read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
#remove any duplicate rows/entries
data <- subset(data, !duplicated(data$event_unique_id))
#remove columns that aren't useful/duplicates
#duplicates of other columns
#UCR codes - not used in this case
#ID number - not needed
#Hood ID - not needed
data <- data[, !colnames(data) %in% c("X","Y","Index_","event_unique_id","ucr_code","ucr","FID","Hood_ID")]
#formatting dates - remove garbage time values at the end
data$occurrencedate <- ymd(gsub("(.*)T.*", "\\1", data$occurrencedate))
data$reporteddate <- ymd(gsub("(.*)T.*", "\\1", data$reporteddate))
#removing whitespace from day of week
data$occurrencedayofweek <- as.factor(trimws(data$occurrencedayofweek, "b"))
data$reporteddayofweek <- as.factor(trimws(data$reporteddayofweek, "b"))
#missing data
colSums(is.na(data))
lapply (data, function (x) which (is.na (x)))
unlist (lapply (data, function (x) which (is.na (x))))
unique (unlist (lapply (data, function (x) which (is.na (x)))))
#missing data
colSums(is.na(data))
NAdata <- unique (unlist (lapply (data, function (x) which (is.na (x)))))
NAdata
data$occurrencedate[NAdata]
class(data$occurrencedate[NAdata])
?month
levels(data$occurrencemonth)
data$occurrencemonth[NAdata] <- month(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrenceday[NAdata] <- day(data$occurrencedate[NAdata])
data$occurrencedayofweek[NAdata] <- wday(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrencedayofyear[NAdata] <- yday(data$occurrencedate[NAdata])
str(data)
#change things to factors
for(col in c("reportedyear","reportedday","reporteddayofyear","reportedhour","occurrenceyear","occurrenceday",
"occurrencedayofyear","occurrencehour")) {
data[,col] = as.factor(data[,col])
}
str(data)
data <- read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
str(data)
#remove any duplicate rows/entries
data <- subset(data, !duplicated(data$event_unique_id))
#remove columns that aren't useful/duplicates
#duplicates of other columns
#UCR codes - not used in this case
#ID number - not needed
#Hood ID - not needed
data <- data[, !colnames(data) %in% c("X","Y","Index_","event_unique_id","ucr_code","ucr_ext","FID","Hood_ID")]
#formatting dates - remove garbage time values at the end
data$occurrencedate <- ymd(gsub("(.*)T.*", "\\1", data$occurrencedate))
data$reporteddate <- ymd(gsub("(.*)T.*", "\\1", data$reporteddate))
#removing whitespace from day of week
data$occurrencedayofweek <- as.factor(trimws(data$occurrencedayofweek, "b"))
data$reporteddayofweek <- as.factor(trimws(data$reporteddayofweek, "b"))
#missing data
colSums(is.na(data))
NAdata <- unique (unlist (lapply (data, function (x) which (is.na (x)))))
#imputing occurence dates from occurence date field
data$occurrenceyear[NAdata] <- year(data$occurrencedate[NAdata])
data$occurrencemonth[NAdata] <- month(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrenceday[NAdata] <- day(data$occurrencedate[NAdata])
data$occurrencedayofweek[NAdata] <- wday(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrencedayofyear[NAdata] <- yday(data$occurrencedate[NAdata])
#change things to factors
for(col in c("reportedyear","reportedday","reporteddayofyear","reportedhour","occurrenceyear","occurrenceday",
"occurrencedayofyear","occurrencehour")) {
data[,col] = as.factor(data[,col])
}
#drop unused factor levels
data$occurrencedayofweek <- droplevels(data$occurrencedayofweek)
str(data)
levels(data$occurrencedayofweek)
#drop unused factor levels
data$occurrencedayofweek <- droplevels(data$occurrencedayofweek)
levels(data$occurrencedayofweek)
levels(data$occurrencemonth)
data$occurrencemonth <- droplevels(data$occurrencemonth)
levels(data$occurrencemonth)
names(data)
#drop unused factor levels
for(col in names(data)) {
if(is.factor(data[,col])) {
data[,col] = droplevels(data[,col])
}
}
str(data)
library(tidyr)
library(dplyr)
library(cluster)
#neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Neighbourhood)
library(tidyr)
library(dplyr)
library(cluster)
#neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Neighbourhood)
bygroup
groups <- summarise(bygroup, n=n())
groups
groups <- groups[c("Neighbourhood", "MCI", "n")]
groups
hood <- spread(groups, key=MCI, value=n)
hood
hood <- hood[, -1]
hood
str(data)
str(data$MCI)
levels(data$MCI)
#neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Neighbourhood)
groups <- summarise(bygroup, n=n())
groups <- groups[c("Neighbourhood", "MCI", "n")]
groups
levels(groups$MCI)
hood <- spread(groups, key=MCI, value=n)
str(good)
str(hood)
str(data)
?gsub
#replace space in "offence"
data$offence <- gsub("\s", "_", data$offence)
#replace space in "offence"
data$offence <- gsub("\\s", "_", data$offence)
#drop unused factor levels
for(col in names(data)) {
if(is.factor(data[,col])) {
data[,col] = droplevels(data[,col])
}
}
#neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Neighbourhood)
groups <- summarise(bygroup, n=n())
groups <- groups[c("Neighbourhood", "MCI", "n")]
hood <- spread(groups, key=MCI, value=n)
hood <- hood[, -1]
hood
data$offence
str(data)
data$MCI <- gsub("\\s", "_", data$MCI)
#neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Neighbourhood)
groups <- summarise(bygroup, n=n())
groups <- groups[c("Neighbourhood", "MCI", "n")]
hood <- spread(groups, key=MCI, value=n)
hood <- hood[, -1]
hood
for(col in names(hood)) {
hood[,col] = ztransform(hood[,col])
}
#normalize data using z-transformation
ztransform <- function (data){
(data - mean(data)) / sd(data)
}
for(col in names(hood)) {
hood[,col] = ztransform(hood[,col])
}
hood
#normalize data using z-transformation
ztransform <- function (data){
(data - mean(data)) / sd(data)
}
for(col in names(hood)) {
hood[,col] = ztransform(hood[,col])
}
names(hood)
hood[,"Assault"]
class(hood[,"Assault"])
hood <- data.frame(hood[, -1])
#normalize data using z-transformation
ztransform <- function (data){
(data - mean(data)) / sd(data)
}
for(col in names(hood)) {
hood[,col] = ztransform(hood[,col])
}
hood
#determine number of clusters
wssplot <- function(data, nc=15, seed=1234) {
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
?var
sum(apply(hood,2,var))
apply(hood,2,var)
(nrow(hood)-1)*sum(apply(hood,2,var))
(nrow(hood)-1)
?kmeans
kmeans(hood, centers=i)$withinss
kmeans(hood, centers=3)$withinss
kmeans(hood, centers=4)$withinss
#we can see there's an elbow around 3 clusters
wssplot(hood, nc=15)
sum(kmeans(hood, centers=3)$withinss)
kmeans(hood, centers=3)$withinss
?kmeans
#determine number of clusters
wssplot <- function(data, nc=15, seed=1234) {
#wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc) {
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)
}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
#we can see there's an elbow around 3 clusters
wssplot(hood, nc=15)
?apply
x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
x
apply(x, 2, sort)
# k-means
k.means.fit <- kmeans(hood, 3)
k.means.fit
?clusplot
k.means.fit$cluster
?clusplot
#plotting k-means
clusplot(hood, k.means.fit$cluster, main='2D representation of the Cluster solution',
color=TRUE, shade=TRUE,
labels=2, lines=0)
?dist
#hierarchical
d <- dist(hood, method = "euclidean") # Euclidean distance matrix.
d
hclust
?hclust
H.fit <- hclust(d, method="ward.D2")
#plot dendrogram
plot(H.fit) # display dendogram
groups <- cutree(H.fit, k=3) # cut tree into 3 clusters
# draw dendogram with red borders around the 3 clusters
rect.hclust(H.fit, k=3, border="red")
# if we want to look at other numbers of clusters
counts <- sapply(2:6, function(ncl)table(cutree(H.fit, ncl)))
names(counts) <- 2:6
counts
library(lubridate)
data_dir <- "data"
data <- read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
#remove any duplicate rows/entries
data <- subset(data, !duplicated(data$event_unique_id))
#remove columns that aren't useful/duplicates
#duplicates of other columns, UCR codes - not used in this case, ID number - not needed, Hood ID - not needed
data <- data[, !colnames(data) %in% c("X","Y","Index_","event_unique_id","ucr_code","ucr_ext","FID","Hood_ID")]
#formatting dates - remove garbage time values at the end
data$occurrencedate <- ymd(gsub("(.*)T.*", "\\1", data$occurrencedate))
data$reporteddate <- ymd(gsub("(.*)T.*", "\\1", data$reporteddate))
#removing whitespace from day of week
data$occurrencedayofweek <- as.factor(trimws(data$occurrencedayofweek, "b"))
data$reporteddayofweek <- as.factor(trimws(data$reporteddayofweek, "b"))
#missing data
#colSums(is.na(data))
NAdata <- unique (unlist (lapply (data, function (x) which (is.na (x)))))
#imputing occurence dates from occurence date field
data$occurrenceyear[NAdata] <- year(data$occurrencedate[NAdata])
data$occurrencemonth[NAdata] <- month(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrenceday[NAdata] <- day(data$occurrencedate[NAdata])
data$occurrencedayofweek[NAdata] <- wday(data$occurrencedate[NAdata], label = TRUE, abbr = FALSE)
data$occurrencedayofyear[NAdata] <- yday(data$occurrencedate[NAdata])
#change things to factors
for(col in c("reportedyear","reportedday","reporteddayofyear","reportedhour","occurrenceyear","occurrenceday",
"occurrencedayofyear","occurrencehour")) {
data[,col] = as.factor(data[,col])
}
#replace space in string
data$offence <- gsub("\\s", "_", data$offence)
data$MCI <- gsub("\\s", "_", data$MCI)
#drop unused factor levels
for(col in names(data)) {
if(is.factor(data[,col])) {
data[,col] = droplevels(data[,col])
}
}
# Check missing values one last time
colSums(is.na(data))
library(tidyr)
library(dplyr)
library(cluster)
# Neighbourhood first
#first, coerce the data into a table that can be clustered - we aren't interested in the occurence date at this point
#courtesy of Susan Li - https://datascienceplus.com/exploring-clustering-and-mapping-torontos-crimes/
bygroup <- group_by(data, MCI, Neighbourhood)
groups <- summarise(bygroup, n=n())
groups <- groups[c("Neighbourhood", "MCI", "n")]
hood <- spread(groups, key=MCI, value=n)
hood <- data.frame(hood[, -1])
#normalize data
for(col in names(hood)) {
hood[,col] = (hood[,col] - mean(hood[,col])) / sd(hood[,col])
}
#determine number of clusters
wssplot <- function(data, nc=15, seed=1234) {
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc) {
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)
}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
}
#we can see there's an elbow around 3 clusters
wssplot(hood, nc=15)
# k-means
k.means.fit <- kmeans(hood, 3)
k.means.fit
#cluster 3 has lower than average crime, while cluster 2 has higher than average. cluster 1 sits in the middle for incidents
#cluster 3 is also the one with the most number of neighbourhoods which implies that the majority of toronto is very safe
#there are only 10 neighbourhoods in cluster 2, which means crime is concentrated in these small pockets of toronto
#plotting k-means
clusplot(hood, k.means.fit$cluster, main='2D representation of the Cluster solution',
color=TRUE, shade=TRUE,
labels=2, lines=0)
#we can see there is some overlap between cluster 1 and 3, which means they could potentially be one cluster
#implies that they are more similar to each other than they are to cluster 2
#hierarchical
d <- dist(hood, method = "euclidean") # Euclidean distance matrix.
H.fit <- hclust(d, method="ward.D2")
#plot dendrogram
plot(H.fit) # display dendogram
groups <- cutree(H.fit, k=3) # cut tree into 3 clusters
# draw dendogram with red borders around the 3 clusters
rect.hclust(H.fit, k=3, border="red")
# if we want to look at other numbers of clusters
counts <- sapply(2:6, function(ncl) table(cutree(H.fit, ncl)))
names(counts) <- 2:6
counts
# we can see that 3 clusters is likely ideal because we don't want to split many clusters with small numbers of neighbourhoods
library(dplyr)
library(ggplot2)
library(dplyr)
library(ggmap)
# Note: To run this script and to use google's map service, api keys needs to be set up properly. Refer to the following link: https://stackoverflow.com/questions/36175529/getting-over-query-limit-after-one-request-with-geocode
data = read.csv("data/MCI_2014_to_2017.csv", header=T, sep=",", na.strings = "NA")
data = select(data,c(-X,-Y))
data = data[!duplicated(data$event_unique_id),]
sapply(data, function(x) {sum(is.na(x))/length(x)*100})
summary(data)
ggplot(data[data$occurrenceyear>=2012,],aes(x=occurrenceyear,fill=MCI)) + geom_bar() +
theme(axis.text.x=element_text(angle = -90, hjust = 0))
lat <- data$Lat
lon <- data$Long
crimes <- data$MCI
to_map <- data.frame(crimes, lat, lon)
colnames(to_map) <- c('crimes', 'lat', 'lon')
sbbox <- make_bbox(lon = data$Long, lat = data$Lat, f = 0.1)
my_map <- get_map(location = sbbox, maptype = "roadmap", scale = 2, color="bw", zoom = 10)
register_google(key = "AIzaSyAA4Odo-jCEuqZuakvUadhqg8_Z-bfXgFM")
lat <- data$Lat
lon <- data$Long
crimes <- data$MCI
to_map <- data.frame(crimes, lat, lon)
colnames(to_map) <- c('crimes', 'lat', 'lon')
sbbox <- make_bbox(lon = data$Long, lat = data$Lat, f = 0.1)
my_map <- get_map(location = sbbox, maptype = "roadmap", scale = 2, color="bw", zoom = 10)
ggmap(my_map) +
geom_point(data=to_map, aes(x = lon, y = lat, color = "#27AE60"),
size = 0.5, alpha = 0.03) +
xlab('Longitude') +
ylab('Latitude') +
ggtitle('Location of Major Crime Indicators Toronto(2014-2017 total)') +
guides(color=FALSE)
p = ggplot(data, aes(factor(occurrencehour), factor(MCI))) + geom_bin2d() +
scale_fill_gradient(low = "white", high = "red") +
ylab("MCI") +
xlab("Hours") +
theme(legend.title = element_text(size = 10),
legend.text = element_text(size = 12),
plot.title = element_text(size=16),
axis.title=element_text(size=14,face="bold"),
axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(fill = "# of Occurance")
newdat <- ggplot_build(p)$data[[1]]
p + geom_text(data=newdat, aes((xmin + xmax)/2, (ymin + ymax)/2, label=count), col="black", angle = 90)
library(maptools)
shpfile <- "/Users/james/edu/YorkU_ML/term1/assigment2/crime/NEIGHBORHOODS_WGS84_2.shp"
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% summarise(offence_cnt = n())
hood_total_offence_cnt_table = merge(total_offence_cnt_table,sh@data,by.x='Hood_ID',by.y='AREA_S_CD')
points <- fortify(sh, region = 'AREA_S_CD')
points2 <- merge(points, hood_total_offence_cnt_table, by.x='id', by.y='Hood_ID', all.x=TRUE)
toronto <- qmap("Toronto, Ontario", zoom=11)
toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points2, color='black') +
scale_fill_gradient(low='white', high='red')
library("RColorBrewer")
toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points2, color='black') +
scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
library(maptools)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% summarise(offence_cnt = n())
hood_total_offence_cnt_table = merge(total_offence_cnt_table,sh@data,by.x='Hood_ID',by.y='AREA_S_CD')
points <- fortify(sh, region = 'AREA_S_CD')
toronto <- qmap("Toronto, Ontario", zoom=11)
toronto <- qmap("Toronto, Ontario", zoom=11)
toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points2, color='black') +
scale_fill_gradient(low='white', high='red')
library("RColorBrewer")
toronto + geom_polygon(aes(x=long,y=lat, group=group, fill=offence_cnt), data=points2, color='black') +
scale_fill_distiller(palette='Spectral') + scale_alpha(range=c(0.5,0.5))
library(ggplot2)
library(plyr)
library(dplyr)
library(Hmisc)
library(ggpubr)
library(DMwR)
library(caret)
library(pROC)
model_dir = "models"
data_dir = "data"
#saved_models = list.files(model_dir)
#for(file in saved_models) {
#  load(paste(model_dir,file,sep="/"))
#}
data=read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
library(maptools)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% summarise(offence_cnt = n())
library(ggplot2)
library(dplyr)
model_dir = "models"
data_dir = "data"
#saved_models = list.files(model_dir)
#for(file in saved_models) {
#  load(paste(model_dir,file,sep="/"))
#}
data=read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
library(maptools)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% summarise(offence_cnt = n())
library(ggplot2)
library(plyr)
model_dir = "models"
data_dir = "data"
#saved_models = list.files(model_dir)
#for(file in saved_models) {
#  load(paste(model_dir,file,sep="/"))
#}
data=read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
library(maptools)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% summarise(offence_cnt = n())
library(ggplot2)
library(plyr)
model_dir = "models"
data_dir = "data"
#saved_models = list.files(model_dir)
#for(file in saved_models) {
#  load(paste(model_dir,file,sep="/"))
#}
data=read.csv(paste(data_dir,"MCI_2014_to_2017.csv",sep="/"), header = TRUE, sep = ",")
library(maptools)
shpfile <- paste(data_dir,"NEIGHBORHOODS_WGS84_2.shp",sep="/")
sh <- readShapePoly(shpfile)
sh@data$AREA_S_CD <- as.integer(sh@data$AREA_S_CD)
total_offence_cnt_table = data %>% group_by(Hood_ID) %>% summarise(offence_cnt = n())
